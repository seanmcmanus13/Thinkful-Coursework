{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boost guided example\n",
    "\n",
    "Having walked through gradient boost by hand, now let's try it with SKlearn.  We'll still use the European Social Survey Data, but now with a categorical outcome: Whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv((\n",
    "    \"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "    \"master/ESS_practice_data/ESSdata_Thinkful.csv\")).dropna()\n",
    "\n",
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year  tvtot  ppltrst  pplfair  pplhlp  happy  sclmeet  sclact  gndr  \\\n",
      "0        6    3.0      3.0     10.0     5.0    8.0      5.0     4.0   2.0   \n",
      "1        6    6.0      5.0      7.0     5.0    9.0      3.0     2.0   2.0   \n",
      "2        6    1.0      8.0      8.0     8.0    7.0      6.0     3.0   1.0   \n",
      "3        6    4.0      6.0      6.0     7.0   10.0      6.0     2.0   2.0   \n",
      "4        6    5.0      6.0      7.0     5.0    8.0      7.0     2.0   2.0   \n",
      "6        6    3.0      0.0      5.0     2.0    0.0      2.0     2.0   1.0   \n",
      "7        6    2.0      4.0      5.0     3.0   10.0      5.0     2.0   2.0   \n",
      "8        6    2.0      8.0      8.0     8.0    9.0      6.0     4.0   2.0   \n",
      "9        6    4.0      4.0      4.0     8.0    7.0      4.0     2.0   2.0   \n",
      "10       6    1.0      6.0      7.0     7.0    9.0      5.0     2.0   2.0   \n",
      "11       6    4.0      5.0      7.0     7.0    7.0      5.0     3.0   1.0   \n",
      "12       6    4.0      7.0      7.0     4.0    9.0      6.0     2.0   1.0   \n",
      "13       6    2.0      1.0      9.0     7.0    8.0      4.0     3.0   1.0   \n",
      "14       6    4.0      4.0      5.0     3.0    8.0      1.0     2.0   2.0   \n",
      "15       6    5.0      4.0      5.0     5.0    9.0      3.0     3.0   2.0   \n",
      "16       6    4.0      5.0      7.0     7.0    8.0      3.0     3.0   1.0   \n",
      "17       6    2.0      7.0      7.0     7.0    6.0      6.0     3.0   1.0   \n",
      "18       6    0.0      9.0      7.0     8.0    9.0      5.0     4.0   1.0   \n",
      "19       6    2.0      6.0      6.0     6.0    8.0      4.0     3.0   2.0   \n",
      "20       6    7.0      3.0      4.0     4.0    7.0      6.0     4.0   1.0   \n",
      "21       6    1.0      8.0      8.0     6.0    8.0      7.0     4.0   1.0   \n",
      "22       6    2.0      7.0      7.0     8.0    9.0      6.0     3.0   1.0   \n",
      "23       6    2.0      4.0      8.0     8.0    9.0      6.0     2.0   2.0   \n",
      "24       6    0.0      7.0      7.0     4.0    9.0      4.0     2.0   2.0   \n",
      "25       6    0.0      6.0      7.0     5.0    7.0      6.0     2.0   2.0   \n",
      "26       6    2.0      7.0      6.0     5.0    8.0      4.0     3.0   2.0   \n",
      "27       6    0.0      5.0      8.0     6.0    9.0      4.0     2.0   2.0   \n",
      "28       6    2.0      8.0      8.0     8.0   10.0      6.0     3.0   2.0   \n",
      "29       6    7.0      8.0      5.0     5.0    8.0      5.0     3.0   2.0   \n",
      "30       6    6.0      8.0      8.0     3.0    5.0      6.0     1.0   2.0   \n",
      "...    ...    ...      ...      ...     ...    ...      ...     ...   ...   \n",
      "8564     7    2.0      1.0      5.0     4.0   10.0      7.0     4.0   1.0   \n",
      "8565     7    5.0      5.0      3.0     4.0    5.0      6.0     1.0   2.0   \n",
      "8566     7    2.0      7.0      8.0     6.0    8.0      7.0     3.0   2.0   \n",
      "8567     7    2.0      4.0      8.0     7.0    9.0      7.0     4.0   2.0   \n",
      "8568     7    1.0      2.0      8.0     6.0    9.0      7.0     3.0   2.0   \n",
      "8569     7    6.0      5.0      3.0     4.0    9.0      6.0     4.0   1.0   \n",
      "8570     7    1.0      5.0      7.0     7.0    8.0      7.0     4.0   1.0   \n",
      "8571     7    3.0      4.0      9.0     7.0    8.0      6.0     3.0   1.0   \n",
      "8572     7    5.0      7.0      8.0     5.0    8.0      3.0     1.0   1.0   \n",
      "8573     7    1.0      8.0      8.0     6.0    9.0      7.0     3.0   2.0   \n",
      "8574     7    1.0      4.0      5.0     6.0    7.0      7.0     3.0   2.0   \n",
      "8575     7    3.0      8.0      7.0     7.0    8.0      7.0     3.0   1.0   \n",
      "8576     7    4.0      5.0      4.0     4.0    7.0      6.0     3.0   1.0   \n",
      "8577     7    6.0      6.0      9.0     7.0    5.0      4.0     2.0   2.0   \n",
      "8578     7    1.0      5.0      5.0     5.0    7.0      5.0     4.0   2.0   \n",
      "8579     7    2.0      8.0      9.0     7.0    9.0      7.0     3.0   2.0   \n",
      "8580     7    4.0      7.0      8.0     5.0   10.0      4.0     1.0   1.0   \n",
      "8581     7    1.0      6.0      7.0     6.0    8.0      6.0     3.0   1.0   \n",
      "8582     7    6.0      7.0      7.0     6.0    8.0      7.0     3.0   1.0   \n",
      "8583     7    2.0      6.0      8.0     5.0   10.0      6.0     5.0   1.0   \n",
      "8584     7    2.0      8.0     10.0     6.0    9.0      6.0     2.0   1.0   \n",
      "8585     7    1.0      3.0      6.0     4.0    9.0      7.0     3.0   1.0   \n",
      "8586     7    2.0      4.0      6.0     3.0    7.0      7.0     3.0   2.0   \n",
      "8587     7    4.0      4.0      6.0     7.0    9.0      7.0     3.0   1.0   \n",
      "8588     7    1.0      6.0      5.0     5.0   10.0      7.0     2.0   1.0   \n",
      "8589     7    3.0      4.0      5.0     3.0    6.0      6.0     2.0   1.0   \n",
      "8590     7    5.0      6.0      4.0     4.0   10.0      6.0     3.0   1.0   \n",
      "8591     7    4.0      5.0      7.0     6.0    8.0      6.0     3.0   1.0   \n",
      "8592     7    5.0      8.0      8.0     6.0    9.0      7.0     3.0   1.0   \n",
      "8593     7    2.0      6.0      7.0     5.0    7.0      7.0     4.0   2.0   \n",
      "\n",
      "      agea  CH  CZ  DE  ES  NO  SE  \n",
      "0     60.0   1   0   0   0   0   0  \n",
      "1     59.0   1   0   0   0   0   0  \n",
      "2     24.0   1   0   0   0   0   0  \n",
      "3     64.0   1   0   0   0   0   0  \n",
      "4     55.0   1   0   0   0   0   0  \n",
      "6     76.0   1   0   0   0   0   0  \n",
      "7     30.0   1   0   0   0   0   0  \n",
      "8     84.0   1   0   0   0   0   0  \n",
      "9     62.0   1   0   0   0   0   0  \n",
      "10    33.0   1   0   0   0   0   0  \n",
      "11    40.0   1   0   0   0   0   0  \n",
      "12    69.0   1   0   0   0   0   0  \n",
      "13    59.0   1   0   0   0   0   0  \n",
      "14    32.0   1   0   0   0   0   0  \n",
      "15    70.0   1   0   0   0   0   0  \n",
      "16    61.0   1   0   0   0   0   0  \n",
      "17    30.0   1   0   0   0   0   0  \n",
      "18    21.0   1   0   0   0   0   0  \n",
      "19    36.0   1   0   0   0   0   0  \n",
      "20    51.0   1   0   0   0   0   0  \n",
      "21    25.0   1   0   0   0   0   0  \n",
      "22    62.0   1   0   0   0   0   0  \n",
      "23    20.0   1   0   0   0   0   0  \n",
      "24    22.0   1   0   0   0   0   0  \n",
      "25    32.0   1   0   0   0   0   0  \n",
      "26    35.0   1   0   0   0   0   0  \n",
      "27    26.0   1   0   0   0   0   0  \n",
      "28    35.0   1   0   0   0   0   0  \n",
      "29    54.0   1   0   0   0   0   0  \n",
      "30    38.0   1   0   0   0   0   0  \n",
      "...    ...  ..  ..  ..  ..  ..  ..  \n",
      "8564  17.0   0   0   0   0   0   1  \n",
      "8565  17.0   0   0   0   0   0   1  \n",
      "8566  17.0   0   0   0   0   0   1  \n",
      "8567  17.0   0   0   0   0   0   1  \n",
      "8568  17.0   0   0   0   0   0   1  \n",
      "8569  17.0   0   0   0   0   0   1  \n",
      "8570  16.0   0   0   0   0   0   1  \n",
      "8571  16.0   0   0   0   0   0   1  \n",
      "8572  16.0   0   0   0   0   0   1  \n",
      "8573  16.0   0   0   0   0   0   1  \n",
      "8574  16.0   0   0   0   0   0   1  \n",
      "8575  16.0   0   0   0   0   0   1  \n",
      "8576  16.0   0   0   0   0   0   1  \n",
      "8577  16.0   0   0   0   0   0   1  \n",
      "8578  16.0   0   0   0   0   0   1  \n",
      "8579  16.0   0   0   0   0   0   1  \n",
      "8580  16.0   0   0   0   0   0   1  \n",
      "8581  16.0   0   0   0   0   0   1  \n",
      "8582  16.0   0   0   0   0   0   1  \n",
      "8583  16.0   0   0   0   0   0   1  \n",
      "8584  16.0   0   0   0   0   0   1  \n",
      "8585  16.0   0   0   0   0   0   1  \n",
      "8586  15.0   0   0   0   0   0   1  \n",
      "8587  15.0   0   0   0   0   0   1  \n",
      "8588  15.0   0   0   0   0   0   1  \n",
      "8589  18.0   0   0   0   0   0   1  \n",
      "8590  15.0   0   0   0   0   0   1  \n",
      "8591  44.0   0   0   0   0   0   1  \n",
      "8592  15.0   0   0   0   0   0   1  \n",
      "8593  15.0   0   0   0   0   0   1  \n",
      "\n",
      "[8147 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now working with a binary outcome, we've switched to a classifier.  Now our loss function can't be the residuals.  Our options are \"deviance\", or \"exponential\".  Deviance is used for logistic regression, and we'll try that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.04650845608292417\n",
      "Percent Type II errors: 0.17607746863066012\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.06257668711656442\n",
      "Percent Type II errors: 0.18527607361963191\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike decision trees, gradient boost solutions are not terribly easy to interpret on the surface.  But they aren't quite a black box.  We can get a measure of how important various features are by counting how many times a feature is used over the course of many decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that age and happiness are the most important features in predicting whether or not someone lives with a partner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DRILL: Improve this gradient boost model\n",
    "\n",
    "While this model is already doing alright, we've seen from the Type I and Type II error rates that there is definitely room for improvement.  Your task is to see how low you can get the error rates to go in the test set, based on your model in the training set.  Strategies you might use include:\n",
    "\n",
    "* Creating new features\n",
    "* Applying more overfitting-prevention strategies like subsampling\n",
    "* More iterations\n",
    "* Trying a different loss function\n",
    "* Changing the structure of the weak learner: Allowing more leaves in the tree, or other modifications\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partner                 1.000000\n",
      "agea                    0.256670\n",
      "sclmeet                 0.162970\n",
      "agesocialinteraction    0.149124\n",
      "happy                   0.145061\n",
      "ppltrst                 0.034371\n",
      "pplfair                 0.034054\n",
      "gndr                    0.033984\n",
      "tvtot                   0.028816\n",
      "pplhlp                  0.022069\n",
      "Name: partner, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corrmatrix = df.corr()\n",
    "print(corrmatrix['partner'].abs().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partner    1.000000\n",
      "agea       0.256670\n",
      "sclmeet    0.162970\n",
      "happy      0.145061\n",
      "ppltrst    0.034371\n",
      "pplfair    0.034054\n",
      "gndr       0.033984\n",
      "tvtot      0.028816\n",
      "pplhlp     0.022069\n",
      "year       0.018689\n",
      "Name: partner, dtype: float64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agesocialinteraction']=df['agea']*df['sclmeet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partner                 1.000000\n",
      "agea                    0.256670\n",
      "sclmeet                 0.162970\n",
      "agesocialinteraction    0.149124\n",
      "happy                   0.145061\n",
      "ppltrst                 0.034371\n",
      "pplfair                 0.034054\n",
      "gndr                    0.033984\n",
      "tvtot                   0.028816\n",
      "pplhlp                  0.022069\n",
      "Name: partner, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corrmatrix = df.corr()\n",
    "print(corrmatrix['partner'].abs().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definine outcome and predictors.\n",
    "# Set our outcome to 0 and 1.\n",
    "y = df['partner'] - 1\n",
    "X = df.loc[:, ~df.columns.isin(['partner', 'cntry', 'idno'])]\n",
    "\n",
    "# Make the categorical variable 'country' into dummies.\n",
    "X = pd.concat([X, pd.get_dummies(df['cntry'])], axis=1)\n",
    "\n",
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.6)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "Percent Type I errors: 0.002182214948172395\n",
      "Percent Type II errors: 0.02823240589198036\n",
      "\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.11901840490797547\n",
      "Percent Type II errors: 0.17791411042944785\n"
     ]
    }
   ],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(n_estimators=)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#optimize for max depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxdepth=2\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.044189852700491\n",
      "Percent Type II errors: 0.1740316421167485\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.05766871165644172\n",
      "Percent Type II errors: 0.18773006134969325\n",
      "\n",
      "\n",
      "maxdepth=3\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.031642116748499725\n",
      "Percent Type II errors: 0.14361702127659576\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.08834355828220859\n",
      "Percent Type II errors: 0.1754601226993865\n",
      "\n",
      "\n",
      "maxdepth=4\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.016639388979814512\n",
      "Percent Type II errors: 0.10065466448445172\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.09447852760736196\n",
      "Percent Type II errors: 0.17423312883435582\n",
      "\n",
      "\n",
      "maxdepth=5\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.006001091107474086\n",
      "Percent Type II errors: 0.04841789416257501\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.11411042944785275\n",
      "Percent Type II errors: 0.16809815950920245\n",
      "\n",
      "\n",
      "maxdepth=6\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.00027277686852154935\n",
      "Percent Type II errors: 0.00872885979268958\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.11779141104294479\n",
      "Percent Type II errors: 0.1668711656441718\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "md= [2,3,4,5,6]\n",
    "# Initialize and fit the model.\n",
    "for m in md:\n",
    "    clf = ensemble.GradientBoostingClassifier(n_estimators=500, max_depth=m, loss='deviance')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = clf.predict(X_train)\n",
    "    predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "    table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "    table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "    train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "    train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "    test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "    test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "    print((\n",
    "    'maxdepth={}\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    ).format(m, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like max depth = 4 is optimal. After that the model becomes overfit. Let's try to optimize for the number of estimators.\n",
    "\n",
    "# Optimize for estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxdepth=4\n",
      "estimators=400\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.020185488270594652\n",
      "Percent Type II errors: 0.11361156573922532\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.0834355828220859\n",
      "Percent Type II errors: 0.1754601226993865\n",
      "\n",
      "\n",
      "maxdepth=4\n",
      "estimators=500\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.016639388979814512\n",
      "Percent Type II errors: 0.10065466448445172\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.09325153374233129\n",
      "Percent Type II errors: 0.17423312883435582\n",
      "\n",
      "\n",
      "maxdepth=4\n",
      "estimators=600\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.013229678123295145\n",
      "Percent Type II errors: 0.0894708128750682\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.09079754601226994\n",
      "Percent Type II errors: 0.18036809815950922\n",
      "\n",
      "\n",
      "maxdepth=4\n",
      "estimators=700\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.012138570649208947\n",
      "Percent Type II errors: 0.07733224222585924\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.09938650306748466\n",
      "Percent Type II errors: 0.18036809815950922\n",
      "\n",
      "\n",
      "maxdepth=4\n",
      "estimators=800\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.011320240043644299\n",
      "Percent Type II errors: 0.07078559738134206\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.10429447852760736\n",
      "Percent Type II errors: 0.18159509202453988\n",
      "\n",
      "\n",
      "maxdepth=4\n",
      "estimators=900\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.009683578832515003\n",
      "Percent Type II errors: 0.062465902891434805\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.1067484662576687\n",
      "Percent Type II errors: 0.18404907975460122\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_est= [400, 500, 600, 700, 800, 900]\n",
    "# Initialize and fit the model.\n",
    "for n in n_est:\n",
    "    clf = ensemble.GradientBoostingClassifier(n_estimators=n, max_depth=4, loss='deviance')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = clf.predict(X_train)\n",
    "    predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "    table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "    table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "    train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "    train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "    test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "    test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "    print((\n",
    "    'maxdepth={}\\n'\n",
    "    'estimators={}\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    ).format(4, n, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators of 600 give an optimate result here - they substantially reduce type II errors at a cost of marginal overfitting...let's try changing the loss function:\n",
    "\n",
    "# Optimizing for loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lossfunction=deviance\n",
      "maxdepth=4\n",
      "estimators=600\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.013229678123295145\n",
      "Percent Type II errors: 0.0894708128750682\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.09202453987730061\n",
      "Percent Type II errors: 0.18036809815950922\n",
      "\n",
      "\n",
      "lossfunction=exponential\n",
      "maxdepth=4\n",
      "estimators=600\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.01568466993998909\n",
      "Percent Type II errors: 0.1039279869067103\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.09570552147239264\n",
      "Percent Type II errors: 0.1754601226993865\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_list= ['deviance', 'exponential']\n",
    "# Initialize and fit the model.\n",
    "for l in loss_list:\n",
    "    clf = ensemble.GradientBoostingClassifier(n_estimators=600, max_depth=4, loss=l)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predict_train = clf.predict(X_train)\n",
    "    predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "    table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "    table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "    train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "    train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "    test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "    test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "    print((\n",
    "    'lossfunction={}\\n'\n",
    "    'maxdepth={}\\n'\n",
    "    'estimators={}\\n'\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    ).format(l, 4, 600, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we've optimized our model. let's do some cross validation here...\n",
    "I think our high type ii errors on the test set may be a product of low sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lossfunction=exponential\n",
      "maxdepth=4\n",
      "estimators=600\n",
      "Training set accuracy:\n",
      "Percent Type I errors: 0.008183306055646482\n",
      "Percent Type II errors: 0.07405891980360066\n",
      "Test set accuracy:\n",
      "Percent Type I errors: 0.08254065664314207\n",
      "Percent Type II errors: 0.18502608162012887\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier(n_estimators=600, max_depth=4, loss=l)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "'lossfunction={}\\n'\n",
    "'maxdepth={}\\n'\n",
    "'estimators={}\\n'\n",
    "'Training set accuracy:\\n'\n",
    "'Percent Type I errors: {}\\n'\n",
    "'Percent Type II errors: {}\\n'\n",
    "'Test set accuracy:\\n'\n",
    "'Percent Type I errors: {}\\n'\n",
    "'Percent Type II errors: {}\\n\\n'\n",
    ").format(l, 4, 600, train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7324332617367291"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7607362 , 0.69815951, 0.7398773 , 0.73050952, 0.69164619])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion:\n",
    "\n",
    "Here is a summary of the changes that we've made :\n",
    "\n",
    "Training set accuracy:\n",
    "Percent Type I errors: 0.04650845608292417 ----> 0.008\n",
    "Percent Type II errors: 0.17607746863066012 ----> 0.074\n",
    "\n",
    "Test set accuracy:\n",
    "Percent Type I errors: 0.06257668711656442  ---> 0.08\n",
    "Percent Type II errors: 0.18527607361963191 ---> 0.185\n",
    "\n",
    "Our model looks slightly more overfit, but it predicts the whole dataset with greater accuracy."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "59px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
