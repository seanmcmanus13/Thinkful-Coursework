{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re #regular expressions\n",
    "import scipy.stats\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from matplotlib.mlab import PCA as mlabPCA\n",
    "from sklearn  import preprocessing\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "#specifically nlp stuff\n",
    "\n",
    "import re\n",
    "from nltk.corpus import  stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cleaned database\n",
    "music = pd.read_csv(r'C:\\Users\\Sean\\Documents\\Thinkful\\RAP NLP\\CSV\\music_clean_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#needed to generate data for doc2vec\n",
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import spacy\n",
    "import en_core_web_sm  # or any other model you downloaded via spacy download or pip\n",
    "import gensim\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import word2vec\n",
    "nlp = en_core_web_sm.load()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to string\n",
    "music.lyrics = music.lyrics.astype(str)\n",
    "#parse data\n",
    "lyrics = music.lyrics.apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       party people in the place. embrace the bass as...\n",
       "1       here i am    raw. terrorist here to bring trou...\n",
       "2       let it roll get bold i just cant hold. back or...\n",
       "3       talk about ever meeting ms right. but one thin...\n",
       "4       yo on the mic right about now i go by the name...\n",
       "5       mmm mmm mmm. aw yea im with this. im just gonn...\n",
       "6       hear ye hear ye from far and near. the one the...\n",
       "7       funky . ready . funky funky ahhh  . one two on...\n",
       "8        who is the man with the master plan . dj mist...\n",
       "9       grab a hold of yourself open your eyes. get wi...\n",
       "10       dr harrah is mr kane loose in this hospital ....\n",
       "11      here comes the conquering brother that fathoms...\n",
       "12      the kiss of death on a rap pick. then you get ...\n",
       "13      on and on and on and on. me say the beat dont ...\n",
       "14      so out of the mercy of allah. and the law writ...\n",
       "15      well excuse me take a few minutes to mellow ou...\n",
       "16       yo whassup big daddy .  aiyyo whassup red ale...\n",
       "17      he has arrived at the apollo. big  daddy  kane...\n",
       "18       hey yo teddy.  yo.  i ran into a young lady m...\n",
       "19      here we go here we go on the smooth side. lets...\n",
       "20      anything goes when it comes to hoes they go. p...\n",
       "21      yo tone. play me some more pimp shit. . and yo...\n",
       "22      mmmm mmm mmm . .  . . my my my my my my my my ...\n",
       "23      introducing ladies and gentlemen. the young ma...\n",
       "24      chorus all together . . well here we are. unit...\n",
       "25      come get some you little bum. i take the cake ...\n",
       "26      lean on me  . .  big daddy kane a teacher   . ...\n",
       "27      yeah . you know 1990 begins a new decade. sinc...\n",
       "28      move over bacon theres something meatier. a na...\n",
       "29      uhh put your weight on it. uhh and uh prince p...\n",
       "                              ...                        \n",
       "1001    they say. influenced by crime addicted to grin...\n",
       "1002    heaven aint hard to find. all you gotta do is ...\n",
       "1003    at 1225 am wednesday 2pac was on his way into ...\n",
       "1004    a coward dies a thousand deaths. a soldier die...\n",
       "1005    its just me against the world. nothing to lose...\n",
       "1006    i shall not fear no man but god. though i walk...\n",
       "1007    heyyyy heyyayyaahhyy. . . yo mo bee mayn drop ...\n",
       "1008    i wanna dedicate this one to robert yummy sand...\n",
       "1009    oh you thug life is yours . life aint no somet...\n",
       "1010    damn another funeral another motherfucker. lor...\n",
       "1011    you are appreciated. when i was young me and m...\n",
       "1012    keeping it real. . . i take a shot of hennessy...\n",
       "1013    whassup its 2pac can you get away . let me com...\n",
       "1014    here we go we gonna send this one out to the o...\n",
       "1015    who you calling rapist . aint that a bitch. yo...\n",
       "1016    child why you by the window whats wrong daddy ...\n",
       "1017    thats right nigga you gotta get your papers in...\n",
       "1018    hard like an erection. young black male. hard ...\n",
       "1019    you know they got me trapped in this prison of...\n",
       "1020    all you wanted to be a soulja a soulja. all yo...\n",
       "1021     whats up .  yo this scene rollers tried to ja...\n",
       "1022    they claim that im violent. just cause i refus...\n",
       "1023    killing us one by one. in one way or another. ...\n",
       "1024    something wicked this way comes. somesomething...\n",
       "1025     suddenly i see some niggas that i dont like ....\n",
       "1026    ever since you was a peewee down by my knee wi...\n",
       "1027    brendas got a baby. . . i hear brendas got a b...\n",
       "1028    oh shit jumped on my mans dick. heard he had a...\n",
       "1029    rebel   rebel. rebel. rebel   rebel. . . they ...\n",
       "1030    shes a part time mutha. . . meet cindi. shes t...\n",
       "Name: parsed_lyrics, Length: 1031, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music['parsed_lyrics'] = lyrics\n",
    "music['parsed_lyrics'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "music['tokenized_lyrics'] = music.apply(lambda row: nltk.word_tokenize(row['lyrics']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "w = re.compile(\"\\w+\",re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(df):\n",
    "    labeled_sentences = []\n",
    "    for index, datapoint in df.iterrows():\n",
    "        tokenized_words = re.findall(w,datapoint[\"lyrics\"].lower())\n",
    "        labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['SENT_%s' %index]))\n",
    "    return labeled_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(labeled_sentences):\n",
    "    model = Doc2Vec(alpha=0.025, min_alpha=0.025, )\n",
    "    model.build_vocab(labeled_sentences)\n",
    "    for epoch in range(10):\n",
    "        model.train(labeled_sentences, total_examples=1030, epochs=5)\n",
    "        model.alpha -= 0.002 \n",
    "        model.min_alpha = model.alpha\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "labelledlyrics = label_sentences(music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_doc2vec_model(labelledlyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_comments(df,d2v_model):\n",
    "    y = []\n",
    "    comments = []\n",
    "    for i in range(0,df.shape[0]):\n",
    "        label = 'SENT_%s' %i\n",
    "        comments.append(d2v_model.docvecs[label])\n",
    "    df['vectorized_lyrics'] = comments\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('labelledlyrics.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daughters', 0.4074193239212036),\n",
       " ('fathers', 0.3978744149208069),\n",
       " ('mothers', 0.3895375430583954),\n",
       " ('dads', 0.3784688711166382),\n",
       " ('brothers', 0.3761492371559143),\n",
       " ('relationships', 0.36438578367233276),\n",
       " ('lanes', 0.33500170707702637),\n",
       " ('cousins', 0.3325534164905548),\n",
       " ('cheer', 0.3317328095436096),\n",
       " ('pride', 0.32994696497917175)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('sons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 name  duration_ms  popularity  num_markets  \\\n",
      "0  Long Live the Kane     295333.0        26.0         78.0   \n",
      "1         Raw - Remix     344960.0        33.0         78.0   \n",
      "\n",
      "                album  disc_number is_explicit  track_number release_date  \\\n",
      "0  Long Live The Kane          1.0       False           1.0         1988   \n",
      "1  Long Live The Kane          1.0       False           2.0         1988   \n",
      "\n",
      "           artist  ...  acousticness  instrumentalness  liveness  valence  \\\n",
      "0  Big Daddy Kane  ...        0.0556          0.000016     0.180    0.612   \n",
      "1  Big Daddy Kane  ...        0.0186          0.000003     0.109    0.705   \n",
      "\n",
      "     tempo  time_signature                                             lyrics  \\\n",
      "0   99.327             4.0  party people in the place. embrace the bass as...   \n",
      "1  110.681             4.0  here i am    raw. terrorist here to bring trou...   \n",
      "\n",
      "      region                                   tokenized_lyrics  \\\n",
      "0  eastcoast  [party, people, in, the, place, ., embrace, th...   \n",
      "1  eastcoast  [here, i, am, raw, ., terrorist, here, to, bri...   \n",
      "\n",
      "                                   vectorized_lyrics  \n",
      "0  [0.7115336, -2.2607968, 1.423232, 1.2199504, 0...  \n",
      "1  [-1.3222361, -2.607439, 1.1008381, 0.89879847,...  \n",
      "\n",
      "[2 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "#use the function defined above to get the vectors into the dataframe\n",
    "df = vectorize_comments(music,model)\n",
    "df = df.drop('parsed_lyrics', 1)\n",
    "print (df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorlist = df[\"vectorized_lyrics\"].T.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sean\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(25)]\n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#randomize dataset\n",
    "music_random = music.sample(frac=1)\n",
    "rapwords_df1 = music_random.iloc[0:330]\n",
    "rapwords_df2 = music_random.iloc[331:660]\n",
    "rapwords_df3 = music_random.iloc[661:1030]\n",
    "\n",
    "#pick out some random words that are in common among songs\n",
    "rapwords1 = rapwords_df1.iloc[np.random.choice(np.arange(len(rapwords_df1)), 300, False)]\n",
    "rapwords2 = rapwords_df2.iloc[np.random.choice(np.arange(len(rapwords_df2)), 300, False)]\n",
    "rapwords3 = rapwords_df3.iloc[np.random.choice(np.arange(len(rapwords_df3)), 300, False)]\n",
    "# Set up the bags.\n",
    "rapwords1 = rapwords1['parsed_lyrics'].apply(bag_of_words)\n",
    "rapwords2 = rapwords2['parsed_lyrics'].apply(bag_of_words)\n",
    "rapwords3 = rapwords3['parsed_lyrics'].apply(bag_of_words)\n",
    "#extract words from lists in df to a series\n",
    "rapwords1 = rapwords1.T.squeeze()\n",
    "rapwords2 = rapwords2.T.squeeze()\n",
    "rapwords3 = rapwords3.T.squeeze()\n",
    "\n",
    "rapwords1 = rapwords1.apply(pd.Series).stack().reset_index(drop=True)\n",
    "rapwords2 = rapwords2.apply(pd.Series).stack().reset_index(drop=True)\n",
    "rapwords3 = rapwords3.apply(pd.Series).stack().reset_index(drop=True)\n",
    "#need more data...consider including rare words..search for these functions in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the words into master lists\n",
    "rapwords1 = rapwords1.tolist()\n",
    "rapwords2 = rapwords2.tolist()\n",
    "rapwords3 = rapwords3.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return those commond words that can be found in all lists\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "intersection_1_2 = intersection(rapwords1, rapwords2)\n",
    "intersection_1_2_3 = intersection(intersection_1_2, rapwords3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_feature_columns(bow_columns):\n",
    "    bow_df = pd.DataFrame(columns=bow_columns)\n",
    "    return bow_df\n",
    "\n",
    "bow_df = bow_feature_columns(intersection_1_2_3)\n",
    "\n",
    "#add song name and song lyric column\n",
    "bow_df['song'] = music['name']\n",
    "bow_df['lyrics'] = music['lyrics'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letstry(words):\n",
    "    for i, sentence in enumerate(bow_df['lyrics']):\n",
    "        for word in words:\n",
    "            bow_df.loc[i, word] = (bow_df['lyrics'].iloc[i]).count(word)\n",
    "\n",
    "letstry(intersection_1_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     inside wish have feel live    d stick hoe flip feeling  ... dick will  \\\n",
      "0         0    1    2    0    4   75     0   0    0       0  ...    0    1   \n",
      "1         0    0    0    0    0   64     0   0    0       0  ...    0    2   \n",
      "2         0    1    1    1    0   78     1   0    1       0  ...    0    3   \n",
      "3         1    0    1    5    1   95     0   0    0       3  ...    0    2   \n",
      "4         0    0    1    0    0   97     0   0    0       0  ...    0    1   \n",
      "5         0    0    4    0    0  129     0   0    0       0  ...    0    1   \n",
      "6         0    1    0    0    4   69     0   1    0       0  ...    0    3   \n",
      "7         0    0    0    0    0   79     0   0    0       0  ...    0    2   \n",
      "8         0    0    0    1    1   74     0   0    0       0  ...    0    1   \n",
      "9         0    0    0    0    0   57     0   0    0       0  ...    0    1   \n",
      "10        0    0    1    0    2   96     0   0    0       0  ...    0    4   \n",
      "11        0    0    2    0    0   94     1   0    0       0  ...    0    0   \n",
      "12        0    0    1    1    0  146     1   0    0       0  ...    0    1   \n",
      "13        1    0    1    0    1   78     1   0    0       0  ...    0    0   \n",
      "14        0    1    1    0    3   89     0   0    0       0  ...    0    1   \n",
      "15        0    1    0    1    0   62     0   0    0       0  ...    0    1   \n",
      "16        0    0    3    0    2  114     0   0    0       0  ...    0    0   \n",
      "17        0    0    1    4    0   92     0   4    0       0  ...    0    0   \n",
      "18        0    0    3    0    0   89     0   1    0       0  ...    0    1   \n",
      "19        0    0    0    0    2   50     0   1    0       0  ...    0    0   \n",
      "20        0    0    1    1    1   65     0   8    0       0  ...    0    1   \n",
      "21        0    0    0    0    0   14     0   0    0       0  ...    0    0   \n",
      "22        0    0    0    6    0   59     0   0    0       1  ...    0    0   \n",
      "23        0    0    0    0    0    6     0   0    0       0  ...    0    1   \n",
      "24        0    0    0    0    0   96     1   0    0       0  ...    0    0   \n",
      "25        0    0    2    0    0   80     0   0    0       0  ...    0    0   \n",
      "26        0    0    1    0    0   97     0   0    0       0  ...    0    0   \n",
      "27        0    0    1    2    1   63     0   0    0       1  ...    0    0   \n",
      "28        0    0    3    2    0   68     0   0    0       2  ...    0    0   \n",
      "29        0    0    1    0    0  122     1   0    1       0  ...    0    1   \n",
      "...     ...  ...  ...  ...  ...  ...   ...  ..  ...     ...  ...  ...  ...   \n",
      "1001      0    0    3    1    1  146     0   0    0       1  ...    0    0   \n",
      "1002      0    1    4    0    0   89     0   0    0       0  ...    0    0   \n",
      "1003      0    0    1    0    0   45     0   0    0       0  ...    0    0   \n",
      "1004      0    0    0    0    1  110     0   1    0       0  ...    1    0   \n",
      "1005      0    0    0    2    1  112     1   0    0       1  ...    0    3   \n",
      "1006      3    0    0    2    1  112     0   0    0       0  ...    0    1   \n",
      "1007      0    0    0    0    1   90     0   0    0       0  ...    0    4   \n",
      "1008      0    0    4    1    1  133     0   0    0       1  ...    0    3   \n",
      "1009      0    0    1    0    1  108     1   0    0       0  ...    0    2   \n",
      "1010      0    0    0    1    0  135     0   0    0       1  ...    0    1   \n",
      "1011      0    1    0    3    1  134     0   0    0       0  ...    0    1   \n",
      "1012      0    1    1    0    1   74     0   0    0       0  ...    0   25   \n",
      "1013      0    1    2    1    0  101     0   0    0       0  ...    0    2   \n",
      "1014      1    0    0    1    0  129     2   0    1       0  ...    0    0   \n",
      "1015      0    0    1    0    0  112     0   0    0       0  ...    0    0   \n",
      "1016      0    0    0    6    3  150     0   0    0       0  ...    0    2   \n",
      "1017      1    0    0    1    0  106     1   2    0       0  ...    0    2   \n",
      "1018      0    0    0    0    0   35     0   1    0       0  ...    0    0   \n",
      "1019      0    0    0    2    1  118     0   0    0       1  ...    0    0   \n",
      "1020      0    1    1    0    0  106     0   0    0       0  ...    0    0   \n",
      "1021      0    0    1    0    0  106     0   1    0       0  ...    2    0   \n",
      "1022      0    0    1    0    0   93     0   0    0       0  ...    0    2   \n",
      "1023      0    0    2    1    0   79     0   0    0       0  ...    0    4   \n",
      "1024      0    0    0    0    0   68     0   1    0       0  ...    0    0   \n",
      "1025      0    0    0    0    0  113     0   0    2       0  ...    0    0   \n",
      "1026      0    0    0    0    0   70     0   0    0       0  ...    0    0   \n",
      "1027      0    0    0    0    0   63     0   0    0       0  ...    0    0   \n",
      "1028      0    0    0    1    0   85     0   1    2       0  ...    4    1   \n",
      "1029      0    0    0    1    0   72     0   0    0       0  ...    0    0   \n",
      "1030      0    0    0    1    2   88     0   0    1       1  ...    0    1   \n",
      "\n",
      "      be set bitch miss hard hold hurt  \\\n",
      "0     13   1     0    0    4    0    0   \n",
      "1     12   0     0    0    1    1    0   \n",
      "2      8   3     0    0    0    3    0   \n",
      "3     23   0     0    0    5    0    0   \n",
      "4      9   1     0    0    2    1    0   \n",
      "5     21   1     0    1    0    1    1   \n",
      "6     10   0     0    0    1    0    0   \n",
      "7     15   0     0    0    0    0    0   \n",
      "8      4   0     0    0    0    2    0   \n",
      "9     10   0     0    0    0    2    0   \n",
      "10    11   1     0    0    0    0    0   \n",
      "11     9   0     0    0    0    0    0   \n",
      "12    17   0     0    0    1    1    0   \n",
      "13     9   0     0    0    0    0    0   \n",
      "14    15   0     0    0    0    2    0   \n",
      "15     3   0     0    2    1    0    0   \n",
      "16     9   1     0    1    0    0    0   \n",
      "17     4   0     0    1    0    0    0   \n",
      "18    13   1     0    0    0    0    0   \n",
      "19     8   0     0    0    0    1    0   \n",
      "20    13   0     1    0    0    0    0   \n",
      "21     0   0     0    0    0    0    0   \n",
      "22    18   0     0    0    0    0    0   \n",
      "23     0   1     0    0    0    0    0   \n",
      "24    10   1     0    1    1    0    1   \n",
      "25     3   0     0    0    0    1    0   \n",
      "26    18   0     0    0    0    1    0   \n",
      "27     6   0     0    1    1    1    0   \n",
      "28     9   0     0    0    0    0    0   \n",
      "29    13   1     0    2    2    1    0   \n",
      "...   ..  ..   ...  ...  ...  ...  ...   \n",
      "1001  14   0     4    1    9    0    0   \n",
      "1002  17   0     0    2   16    0    0   \n",
      "1003   2   1     0    2    0    0    0   \n",
      "1004   4   0     2    0    0    0    0   \n",
      "1005   7   1     0    0    3    0    0   \n",
      "1006   8   0     0    0    0    0    0   \n",
      "1007  11   1     0    1    2    2    0   \n",
      "1008  18   0     0    1    0    0    0   \n",
      "1009  20   0     4    1    0    0    0   \n",
      "1010   4   0     2    0    1    0    0   \n",
      "1011   3   0     0    0    0    1    0   \n",
      "1012  19   0     0    0    1    1    0   \n",
      "1013  22   0     0    1    2    0    0   \n",
      "1014  31   0     1    0    1    0    0   \n",
      "1015   7   0     2    0    0    0    0   \n",
      "1016  14   0     5    0    0    1    0   \n",
      "1017  20   0     3    2    1    1    0   \n",
      "1018   2   0     0    0    8    0    0   \n",
      "1019  10   0     0    0    0    1    0   \n",
      "1020  39   1     0    0    0    0    0   \n",
      "1021  14   0     1    0    0    0    0   \n",
      "1022  17   0     0    0    0    0    2   \n",
      "1023  16   0     1    0    0    0    0   \n",
      "1024   1   0     0    0    0    0    0   \n",
      "1025  25   0     2    0    0    0    0   \n",
      "1026   7   0     0    0    1    0    0   \n",
      "1027   4   0     0    0    1    0    1   \n",
      "1028   3   1    10    0    1    1    0   \n",
      "1029  26   0     0    0    0    1    0   \n",
      "1030  10   0     2    1    0    0    0   \n",
      "\n",
      "                                                 lyrics  \n",
      "0     party people in the place. embrace the bass as...  \n",
      "1     here i am    raw. terrorist here to bring trou...  \n",
      "2     let it roll get bold i just cant hold. back or...  \n",
      "3     talk about ever meeting ms right. but one thin...  \n",
      "4     yo on the mic right about now i go by the name...  \n",
      "5     mmm mmm mmm. aw yea im with this. im just gonn...  \n",
      "6     hear ye hear ye from far and near. the one the...  \n",
      "7     funky . ready . funky funky ahhh  . one two on...  \n",
      "8      who is the man with the master plan . dj mist...  \n",
      "9     grab a hold of yourself open your eyes. get wi...  \n",
      "10     dr harrah is mr kane loose in this hospital ....  \n",
      "11    here comes the conquering brother that fathoms...  \n",
      "12    the kiss of death on a rap pick. then you get ...  \n",
      "13    on and on and on and on. me say the beat dont ...  \n",
      "14    so out of the mercy of allah. and the law writ...  \n",
      "15    well excuse me take a few minutes to mellow ou...  \n",
      "16     yo whassup big daddy .  aiyyo whassup red ale...  \n",
      "17    he has arrived at the apollo. big  daddy  kane...  \n",
      "18     hey yo teddy.  yo.  i ran into a young lady m...  \n",
      "19    here we go here we go on the smooth side. lets...  \n",
      "20    anything goes when it comes to hoes they go. p...  \n",
      "21    yo tone. play me some more pimp shit. . and yo...  \n",
      "22    mmmm mmm mmm . .  . . my my my my my my my my ...  \n",
      "23    introducing ladies and gentlemen. the young ma...  \n",
      "24    chorus all together . . well here we are. unit...  \n",
      "25    come get some you little bum. i take the cake ...  \n",
      "26    lean on me  . .  big daddy kane a teacher   . ...  \n",
      "27    yeah . you know 1990 begins a new decade. sinc...  \n",
      "28    move over bacon theres something meatier. a na...  \n",
      "29    uhh put your weight on it. uhh and uh prince p...  \n",
      "...                                                 ...  \n",
      "1001  they say. influenced by crime addicted to grin...  \n",
      "1002  heaven aint hard to find. all you gotta do is ...  \n",
      "1003  at 1225 am wednesday 2pac was on his way into ...  \n",
      "1004  a coward dies a thousand deaths. a soldier die...  \n",
      "1005  its just me against the world. nothing to lose...  \n",
      "1006  i shall not fear no man but god. though i walk...  \n",
      "1007  heyyyy heyyayyaahhyy. . . yo mo bee mayn drop ...  \n",
      "1008  i wanna dedicate this one to robert yummy sand...  \n",
      "1009  oh you thug life is yours . life aint no somet...  \n",
      "1010  damn another funeral another motherfucker. lor...  \n",
      "1011  you are appreciated. when i was young me and m...  \n",
      "1012  keeping it real. . . i take a shot of hennessy...  \n",
      "1013  whassup its 2pac can you get away . let me com...  \n",
      "1014  here we go we gonna send this one out to the o...  \n",
      "1015  who you calling rapist . aint that a bitch. yo...  \n",
      "1016  child why you by the window whats wrong daddy ...  \n",
      "1017  thats right nigga you gotta get your papers in...  \n",
      "1018  hard like an erection. young black male. hard ...  \n",
      "1019  you know they got me trapped in this prison of...  \n",
      "1020  all you wanted to be a soulja a soulja. all yo...  \n",
      "1021   whats up .  yo this scene rollers tried to ja...  \n",
      "1022  they claim that im violent. just cause i refus...  \n",
      "1023  killing us one by one. in one way or another. ...  \n",
      "1024  something wicked this way comes. somesomething...  \n",
      "1025   suddenly i see some niggas that i dont like ....  \n",
      "1026  ever since you was a peewee down by my knee wi...  \n",
      "1027  brendas got a baby. . . i hear brendas got a b...  \n",
      "1028  oh shit jumped on my mans dick. heard he had a...  \n",
      "1029  rebel   rebel. rebel. rebel   rebel. . . they ...  \n",
      "1030  shes a part time mutha. . . meet cindi. shes t...  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1031 rows x 382 columns]\n"
     ]
    }
   ],
   "source": [
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "#bow_df = bow_df.drop('d',1)\n",
    "bow_df['region'] = music['region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\display.py:689: UserWarning:\n",
      "\n",
      "Consider using IPython.display.IFrame instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~mcmanusdatascience/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = [go.Histogram(x=bow_df['region'])]\n",
    "\n",
    "nbinsx=12\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Distribution of songs by region',\n",
    "    xaxis=dict(\n",
    "        title='Region'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='# of songs'\n",
    "    ),\n",
    "    bargap=0.1,\n",
    "    bargroupgap=0.1\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='styled histogram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial model with BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow_df.drop('region', 1)\n",
    "X = X.drop('lyrics', 1)\n",
    "Y = bow_df.region.astype(str)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'n_estimators': 200,\n",
    "          'max_depth': 10,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0      eastcoast  midwest  south  westcoast  All\n",
      "region                                              \n",
      "eastcoast         87        6      4         16  113\n",
      "midwest           10       27      1         12   50\n",
      "south              7        4     25         16   52\n",
      "westcoast         11        4      5         75   95\n",
      "All              115       41     35        119  310\n"
     ]
    }
   ],
   "source": [
    "table_test = pd.crosstab(Y_test, predict_test, margins=True)\n",
    "\n",
    "print(table_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0      eastcoast  midwest  nan  south  westcoast  All\n",
      "region                                                   \n",
      "eastcoast        266        0    1      0          0  267\n",
      "midwest            0      136    0      0          0  136\n",
      "nan                1        0    1      0          0    2\n",
      "south              1        0    0     96          0   97\n",
      "westcoast          0        0    0      0        219  219\n",
      "All              268      136    2     96        219  721\n"
     ]
    }
   ],
   "source": [
    "table_train = pd.crosstab(Y_train, predict_train, margins=True)\n",
    "print(table_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial Model with DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = music['vectorized_lyrics']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
